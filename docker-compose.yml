version: "3.9"

services:
  llamacpp-server:
    image: ghcr.io/ggerganov/llama.cpp:server
    ports:
      - 3300:3300
    volumes:
      - ./models:/models
      
    environment:
      LLAMA_ARG_MODEL: /models/${MODEL_NAME}
      LLAMA_ARG_CTX_SIZE: 4096
      LLAMA_ARG_N_PARALLEL: 2
      LLAMA_ARG_ENDPOINT_METRICS: 1
      LLAMA_ARG_PORT: 8080
    tty: true
    command: >
       -m models/${MODEL_NAME} -c 512 --host 0.0.0.0 --port 3300

  backend:
    build: ./backend
    volumes:
      - ./backend/app:/code/app
    ports:
      - 80:80
    depends_on: 
      - llamacpp-server
    tty: true
    command: >
       uvicorn app.main:app --reload --host 0.0.0.0 --port 80
  
  frontend:
    build: ./frontend
    volumes:
      - ./frontend/app:/usr/src/app
      - react_node_modules:/usr/src/app/node_modules
    ports:
      - 3000:3000
    tty: true
    command: >
      sh -c "npm install && npm start"
    environment:
      CHOKIDAR_USEPOLLING: "true"

volumes:
  react_node_modules: